{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def extract_features(H, n_components=50):\n",
    "    \"\"\"\n",
    "    Extract spatial and frequency features from the complex channel matrix H using Complex PCA.\n",
    "    \n",
    "    Parameters:\n",
    "        H (numpy.ndarray): Complex channel matrix with shape (samples, 64, 2, 408).\n",
    "        n_components (int): Number of dimensions to reduce to using PCA.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Reduced feature representation (samples, n_components).\n",
    "    \"\"\"\n",
    "    num_samples, num_antennas, num_user_antennas, num_subcarriers = H.shape\n",
    "\n",
    "    # Step 1: Compute spatial covariance matrix\n",
    "    spatial_features = []\n",
    "    for sample in H:\n",
    "        # Combine user antenna dimensions into a single matrix (64, 816)\n",
    "        reshaped_sample = sample.reshape(num_antennas, -1)  # Shape: (64, 816)\n",
    "        # Compute spatial covariance (64x64)\n",
    "        spatial_cov = np.matmul(reshaped_sample, reshaped_sample.conj().T)\n",
    "        spatial_features.append(spatial_cov.flatten())  # Flatten into 1D array\n",
    "\n",
    "    spatial_features = np.array(spatial_features)  # Shape: (samples, 64*64)\n",
    "\n",
    "    # Step 2: Compute frequency domain features\n",
    "    frequency_features = []\n",
    "    for sample in H:\n",
    "        # Average across antennas and user antennas for each subcarrier (408)\n",
    "        freq_avg = np.mean(np.abs(sample), axis=(0, 1))  # Shape: (408,)\n",
    "        frequency_features.append(freq_avg)\n",
    "\n",
    "    frequency_features = np.array(frequency_features)  # Shape: (samples, 408)\n",
    "\n",
    "    # Step 3: Combine spatial and frequency features\n",
    "    combined_features = np.hstack((spatial_features, frequency_features))  # Shape: (samples, 64*64 + 408)\n",
    "\n",
    "    # Step 4: Apply Complex PCA for dimensionality reduction\n",
    "    # Compute covariance matrix of the combined features\n",
    "    cov_matrix = np.cov(combined_features.T)  # Shape: (features, features)\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    # Sort by eigenvalues in descending order\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    # Select the top n_components eigenvectors\n",
    "    principal_components = eigenvectors[:, :n_components]  # Shape: (features, n_components)\n",
    "    # Project data onto the principal components\n",
    "    reduced_features = np.dot(combined_features, principal_components)  # Shape: (samples, n_components)\n",
    "\n",
    "    return reduced_features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_real_amplitude_phase(H):\n",
    "    \"\"\"\n",
    "    Convert complex data to a real-valued matrix using amplitude and phase.\n",
    "    \n",
    "    Parameters:\n",
    "        H (numpy.ndarray): Complex-valued matrix with shape (samples, features).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Real-valued matrix with shape (samples, features * 2).\n",
    "    \"\"\"\n",
    "    amplitude = np.abs(H)  # 振幅\n",
    "    phase = np.angle(H)    # 相位\n",
    "    return np.hstack((amplitude, phase))\n",
    "\n",
    "# 使用示例\n",
    "real_features = convert_to_real_amplitude_phase(H)\n",
    "\n",
    "def reduce_dimension_mds(features, n_components=2):\n",
    "    \"\"\"\n",
    "    使用 MDS 将高维数据降维到 2D。\n",
    "    \n",
    "    Parameters:\n",
    "        features (numpy.ndarray): 高维特征矩阵，形状为 (samples, n_features)。\n",
    "        n_components (int): 降维的目标维度，通常为 2。\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: 降维后的坐标，形状为 (samples, n_components)。\n",
    "    \"\"\"\n",
    "    # Step 1: 计算距离矩阵（使用欧氏距离作为相似性度量）\n",
    "    distance_matrix = np.linalg.norm(features[:, np.newaxis] - features, axis=2)\n",
    "\n",
    "    # Step 2: 使用 MDS 进行降维\n",
    "    mds = MDS(n_components=n_components, dissimilarity=\"precomputed\", random_state=0)\n",
    "    low_dim_coords = mds.fit_transform(distance_matrix)  # Shape: (samples, n_components)\n",
    "    \n",
    "    return low_dim_coords\n",
    "\n",
    "# 示例用法\n",
    "low_dim_coords = reduce_dimension_mds(features)  # 输入 (20000, 100) 的特征\n",
    "\n",
    "\n",
    "\n",
    "# This funcation calculates the positions of all channels, should be implemented by the participants\n",
    "def calcLoc(H, anch_pos, bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num):\n",
    "    '''\n",
    "    H: channel data 信道数据 (20000, 100448)\n",
    "    anch_pos: anchor ID and its coordinates 锚点ID和坐标 (2000, 2)\n",
    "    bs_pos: coordinate of the base station 基站坐标 ([0, 0, 30])\n",
    "    tol_samp_num: total number of channel data points 总样本数 (20000)\n",
    "    anch_samp_num: total number of anchor points 锚点样本数 (2000)\n",
    "    port_num: number of SRS Ports (number of antennas for the UE) UE天线数 (2)\n",
    "    ant_num: number of antennas for the base station 基站天线数 (64)\n",
    "    sc_num: number of subcarriers 子载波数 (408)\n",
    "    '''\n",
    "    ######### The following should be implemented by the participants ################\n",
    "\n",
    "    # Feature Extraction\n",
    "    # 从原始信道数据中提取与位置相关的特征（如角度、延迟等）\n",
    "    # TODO: Extract position-relevant features such as angle of arrival (AoA), time of arrival (ToA), or delay spread from the raw channel data.\n",
    "    # TODO: Apply preprocessing to mitigate Dataset2 impairments, such as AWGN and timing advance.\n",
    "    \n",
    "\n",
    "    # Similarity/Distance Metric\n",
    "    # 定义适当的相似度或距离度量，用于评估不同样本间的相关性。\n",
    "    # TODO: Define appropriate metrics, such as Euclidean distance or cosine similarity, to measure correlation between channel features.\n",
    "\n",
    "    # Dimensionality Reduction Mapping\n",
    "    # 将高维信道特征映射到较低维的虚拟坐标空间，保持相似信道的相对位置关系。\n",
    "    # TODO: Use dimensionality reduction techniques (e.g., PCA, t-SNE, or UMAP) to map high-dimensional features to a low-dimensional space.\n",
    "\n",
    "    # Position Prediction\n",
    "    # 利用降维后的虚拟坐标结合锚点真实位置信息，估算未知位置的坐标。\n",
    "    # TODO: Implement an algorithm (e.g., k-NN, regression, or neural networks) to estimate unknown coordinates using virtual coordinates and anchor positions.\n",
    "\n",
    "    # Post-process results to improve accuracy\n",
    "    # TODO: Apply geometric constraints or filter outliers in predictions.\n",
    "\n",
    "\n",
    "    # Initialize result array\n",
    "    loc_result = np.zeros([tol_samp_num, 2], 'float')\n",
    "    return loc_result\n",
    "\n",
    "# Read in the configuration file\n",
    "def read_cfg_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_fmt = [line.rstrip('\\n').split(' ') for line in lines]\n",
    "    info = line_fmt\n",
    "    bs_pos = list([float(info[0][0]), float(info[0][1]), float(info[0][2])])\n",
    "    tol_samp_num = int(info[1][0])\n",
    "    anch_samp_num = int(info[2][0])\n",
    "    port_num = int(info[3][0])\n",
    "    ant_num = int(info[4][0])\n",
    "    sc_num = int(info[5][0])\n",
    "    return bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num\n",
    "\n",
    "# Read in the info related to the anchor points\n",
    "def read_anch_file(file_path, anch_samp_num):\n",
    "    anch_pos = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_fmt = [line.rstrip('\\n').split(' ') for line in lines]\n",
    "    for line in line_fmt:\n",
    "        tmp = np.array([int(line[0]), float(line[1]), float(line[2])])\n",
    "        if np.size(anch_pos) == 0:\n",
    "            anch_pos = tmp\n",
    "        else:\n",
    "            anch_pos = np.vstack((anch_pos, tmp))\n",
    "    return anch_pos\n",
    "\n",
    "# The channel file is large, read in channels in smaller slices\n",
    "def read_slice_of_file(file_path, start, end):\n",
    "    with open(file_path, 'r') as file:\n",
    "        slice_lines = list(itertools.islice(file, start, end))\n",
    "    return slice_lines\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"<<< Welcome to 2024 Wireless Algorithm Contest! >>>\\n\")\n",
    "    ## For ease of data managenment, input data for different rounds are stored in different folders. Feel free to define your own\n",
    "    # PathSet = {0: \"./Test\", 1: \"./Dataset0\", 2: \"./CompetitionData2\", 3: \"./CompetitionData3\"}\n",
    "    # PrefixSet = {0: \"Round0\", 1: \"Round1\", 2: \"Round2\", 3: \"Round3\"}\n",
    "    PathSet = {0: \"./Test\", 1: \"./Dataset0\", 2: \"./Dataset1\", 3: \"./Dataset2\"}\n",
    "    PrefixSet = {1: \"Dataset0\", 2: \"Dataset1\", 3: \"Dataset2\"}\n",
    "\n",
    "    Ridx = 1  # Flag defining the round of the competition, used for define where to read data。0:Test; 1: 1st round; 2: 2nd round ...\n",
    "    PathRaw = PathSet[Ridx]\n",
    "    Prefix = PrefixSet[Ridx]\n",
    "    \n",
    "    ### Get all files in the folder related to the competition. Data for other rounds should be kept in a different folder  \n",
    "    files = os.listdir(PathRaw)\n",
    "    names = []\n",
    "    for f in sorted(files):\n",
    "        if f.find('CfgData') != -1 and f.endswith('.txt'):\n",
    "            names.append(f.split('CfgData')[-1].split('.txt')[0])\n",
    "    \n",
    "    \n",
    "    for na in names:\n",
    "        FileIdx = int(na)\n",
    "        print('Processing Round ' + str(Ridx) + ' Case ' + str(na))\n",
    "        \n",
    "        # Read in the configureation file: RoundYCfgDataX.txt\n",
    "        print('Loading configuration data file')\n",
    "        cfg_path = PathRaw + '/' + Prefix + 'CfgData' + na + '.txt'\n",
    "        bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num = read_cfg_file(cfg_path)\n",
    "                \n",
    "        # Read in info related to the anchor points: RoundYInputPosX.txt\n",
    "        print('Loading input position file')\n",
    "        anch_pos_path = PathRaw + '/' + Prefix + 'InputPos' + na + '.txt'\n",
    "        anch_pos = read_anch_file(anch_pos_path, anch_samp_num)\n",
    "\n",
    "        # Read in channel data:  RoundYInputDataX.txt\n",
    "        slice_samp_num = 1000  # number of samples in each slice\n",
    "        slice_num = int(tol_samp_num / slice_samp_num)  # total number of slices\n",
    "        csi_path = PathRaw + '/' + Prefix + 'InputData' + na + '.txt'\n",
    "        H = []\n",
    "        for slice_idx in range(2): # range(slice_num): # Read in channel data in a loop. In each loop, only one slice of channel is read in\n",
    "            print('Loading input CSI data of slice ' + str(slice_idx))\n",
    "            slice_lines = read_slice_of_file(csi_path, slice_idx * slice_samp_num, (slice_idx + 1) * slice_samp_num)\n",
    "            Htmp = np.loadtxt(slice_lines)\n",
    "            Htmp = np.reshape(Htmp, (slice_samp_num, 2, sc_num, ant_num, port_num))\n",
    "            Htmp = Htmp[:, 0, :, :, :] + 1j * Htmp[:, 1, :, :, :]\n",
    "            Htmp = np.transpose(Htmp, (0, 3, 2, 1))  # Htmp: (slice_samp_num, ant_num, sc_num, port_num)\n",
    "            if np.size(H) == 0:\n",
    "                H = Htmp\n",
    "            else:\n",
    "                H = np.concatenate((H, Htmp), axis=0)\n",
    "        H = H.astype(np.complex64) # trunc to complex64 to reduce storage\n",
    "        \n",
    "        csi_file = PathRaw + '/' + Prefix + 'InputData' + na + '.npy'\n",
    "        np.save(csi_file, H) # After reading the file, you may save txt file into npy, which is faster for python to read \n",
    "        # H = np.load(csi_file) # if saved in npy, you can load npy file instead of txt\n",
    "        \n",
    "        tStart = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        print('Calculating localization results')\n",
    "        result = calcLoc(H, anch_pos, bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num) # This function should be implemented by yourself\n",
    "        \n",
    "        # Replace the position information for anchor points with ground true coordinates\n",
    "        for idx in range(anch_samp_num):\n",
    "            rowIdx = int(anch_pos[idx][0] - 1)\n",
    "            result[rowIdx] = np.array([anch_pos[idx][1], anch_pos[idx][2]])\n",
    "\n",
    "        # Output, be careful with the precision\n",
    "        print('Writing output position file')\n",
    "        with open(PathRaw + '/' + Prefix + 'OutputPos' + na + '.txt', 'w') as f:\n",
    "            np.savetxt(f, result, fmt='%.4f %.4f')\n",
    "\n",
    "        # This help to evaluate the running time, can be removed!\n",
    "        tEnd = time.perf_counter()\n",
    "        print(\"Total time consuming = {}s\\n\\n\".format(round(tEnd - tStart, 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
