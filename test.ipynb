{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_features_single(sample, n_components=15):  # 增加n_components\n",
    "    num_antennas = sample.shape[0]\n",
    "    reshaped_sample = sample.reshape(num_antennas, -1)\n",
    "    magnitude = np.abs(reshaped_sample)\n",
    "    phase = np.angle(reshaped_sample)\n",
    "    \n",
    "    # 1. 提取更多SVD特征\n",
    "    svd_mag = TruncatedSVD(n_components=n_components)\n",
    "    svd_phase = TruncatedSVD(n_components=n_components)\n",
    "    magnitude_features = svd_mag.fit_transform(magnitude).flatten()\n",
    "    phase_features = svd_phase.fit_transform(phase).flatten()\n",
    "    \n",
    "    # 2. 增强空间相关性特征\n",
    "    corr_matrix_mag = np.abs(np.corrcoef(magnitude))\n",
    "    corr_matrix_phase = np.abs(np.corrcoef(phase))\n",
    "    spatial_features_mag = corr_matrix_mag[np.triu_indices(corr_matrix_mag.shape[0], k=1)]\n",
    "    spatial_features_phase = corr_matrix_phase[np.triu_indices(corr_matrix_phase.shape[0], k=1)]\n",
    "    \n",
    "    # 3. 统计特征\n",
    "    mag_stats = np.concatenate([\n",
    "        np.mean(magnitude, axis=1),\n",
    "        np.std(magnitude, axis=1),\n",
    "        np.max(magnitude, axis=1),\n",
    "        np.min(magnitude, axis=1)\n",
    "    ])\n",
    "    \n",
    "    phase_stats = np.concatenate([\n",
    "        np.mean(phase, axis=1),\n",
    "        np.std(phase, axis=1),\n",
    "        np.max(phase, axis=1),\n",
    "        np.min(phase, axis=1)\n",
    "    ])\n",
    "    \n",
    "    return np.concatenate([\n",
    "        magnitude_features,  # 主要SVD特征\n",
    "        phase_features,     # 主要SVD特征\n",
    "        spatial_features_mag,  # 空间相关性\n",
    "        spatial_features_phase,\n",
    "        mag_stats,     # 辅助统计特征\n",
    "        phase_stats\n",
    "    ])\n",
    "\n",
    "def extract_features(H, n_jobs=-1):\n",
    "    \"\"\"Parallel feature extraction for all samples\"\"\"\n",
    "    features = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(extract_features_single)(sample) \n",
    "        for sample in H\n",
    "    )\n",
    "    return np.array(features)\n",
    "\n",
    "def residual_block(x, units):\n",
    "    \"\"\"Improved residual block with batch normalization\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # Ensure the shortcut has the same dimensions as the main path\n",
    "    if int(shortcut.shape[-1]) != units:\n",
    "        shortcut = Dense(units)(shortcut)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units)(x)\n",
    "    \n",
    "    # Add skip connection\n",
    "    x = tf.keras.layers.Add()([shortcut, x])\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_model(input_dim):\n",
    "    \"\"\"Build an improved model with proper residual connections\"\"\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Initial dense layer\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    \n",
    "    # Final prediction layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(2)(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    \"\"\"更专注于精确定位的损失函数\"\"\"\n",
    "    # 调整权重以更加关注x坐标\n",
    "    weights = tf.constant([1.2, 0.8])  # x坐标权重更大\n",
    "    squared_diff = tf.square(y_true - y_pred)\n",
    "    \n",
    "    # 添加距离惩罚项\n",
    "    distance_error = tf.sqrt(tf.reduce_sum(squared_diff, axis=1))\n",
    "    \n",
    "    # 组合损失：更注重点到点的精确定位\n",
    "    return tf.reduce_mean(weights * squared_diff) + 0.1 * tf.reduce_mean(distance_error)\n",
    "\n",
    "def calcLoc(H, anch_pos, bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num):\n",
    "    # Reshape H to combine port dimensions if needed\n",
    "    if len(H.shape) == 4:  # If H has shape (samples, antennas, subcarriers, ports)\n",
    "        H = H.reshape(H.shape[0], H.shape[1], -1)\n",
    "    \n",
    "    # Extract features in parallel\n",
    "    print(\"Extracting features...\")\n",
    "    features = extract_features(H)\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_indices = anch_pos[:, 0].astype(int) - 1\n",
    "    X_train = features[train_indices]\n",
    "    y_train = anch_pos[:, 1:]\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_mask = np.ones(features.shape[0], dtype=bool)\n",
    "    test_mask[train_indices] = False\n",
    "    X_test = features[test_mask]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_std = scaler_X.fit_transform(X_train)\n",
    "    X_test_std = scaler_X.transform(X_test)\n",
    "    \n",
    "    # Standardize targets\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_std = scaler_y.fit_transform(y_train)\n",
    "    \n",
    "    # Split training data\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train_std, y_train_std, test_size=0.15, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Build and compile model\n",
    "    print(\"Building and training model...\")\n",
    "    model = build_model(X_train_std.shape[1])\n",
    "    \n",
    "    # 使用固定学习率初始化优化器\n",
    "    initial_lr = 0.001\n",
    "    optimizer = Adam(learning_rate=initial_lr)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=weighted_mse\n",
    "    )\n",
    "    \n",
    "    # Training callbacks with learning rate reduction\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        X_train_split,\n",
    "        y_train_split,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict locations\n",
    "    print(\"Making predictions...\")\n",
    "    predictions_std = model.predict(X_test_std)\n",
    "    predictions = scaler_y.inverse_transform(predictions_std)\n",
    "    \n",
    "    # Prepare final results\n",
    "    final_coords = np.zeros((tol_samp_num, 2))\n",
    "    final_coords[train_indices] = y_train\n",
    "    remaining_indices = np.setdiff1d(np.arange(tol_samp_num), train_indices)\n",
    "    final_coords[remaining_indices] = predictions\n",
    "    \n",
    "    return final_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Round 1 Case 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating localization results\n",
      "Extracting features...\n",
      "Building and training model...\n",
      "Epoch 1/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 3.1540 - val_loss: 1.1329 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5954 - val_loss: 1.1533 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3384 - val_loss: 1.1360 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1982 - val_loss: 1.1549 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1479 - val_loss: 1.1347 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1366 - val_loss: 1.1117 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0111 - val_loss: 1.1212 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9858 - val_loss: 1.1436 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9021 - val_loss: 1.1278 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9070 - val_loss: 1.1504 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8360 - val_loss: 1.1229 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8342 - val_loss: 1.1214 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m51/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7648\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7671 - val_loss: 1.1288 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7339 - val_loss: 1.1227 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7068 - val_loss: 1.1332 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7059 - val_loss: 1.1290 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6727 - val_loss: 1.1283 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6830 - val_loss: 1.1416 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6556 - val_loss: 1.1434 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m52/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6165\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6173 - val_loss: 1.1646 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6167 - val_loss: 1.1574 - learning_rate: 2.5000e-04\n",
      "Making predictions...\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Writing output position file\n",
      "Total time consuming = 87.044s\n",
      "\n",
      "\n",
      "Processing Round 1 Case 2\n",
      "Calculating localization results\n",
      "Extracting features...\n",
      "Building and training model...\n",
      "Epoch 1/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 2.7596 - val_loss: 1.2617 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.5753 - val_loss: 1.2327 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4686 - val_loss: 1.1937 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3147 - val_loss: 1.2277 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1974 - val_loss: 1.2859 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1212 - val_loss: 1.3048 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0097 - val_loss: 1.3079 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0055 - val_loss: 1.2846 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9875 - val_loss: 1.3652 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m49/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9391\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9439 - val_loss: 1.3531 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9463 - val_loss: 1.3289 - learning_rate: 5.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8395 - val_loss: 1.3220 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8315 - val_loss: 1.3365 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8477 - val_loss: 1.3394 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8195 - val_loss: 1.3536 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8417 - val_loss: 1.3520 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m51/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8139\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8152 - val_loss: 1.3526 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7610 - val_loss: 1.3558 - learning_rate: 2.5000e-04\n",
      "Making predictions...\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Writing output position file\n",
      "Total time consuming = 86.32s\n",
      "\n",
      "\n",
      "Processing Round 1 Case 3\n",
      "Calculating localization results\n",
      "Extracting features...\n",
      "Building and training model...\n",
      "Epoch 1/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 2.3622 - val_loss: 1.0990 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5080 - val_loss: 1.0717 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2397 - val_loss: 1.0676 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1426 - val_loss: 1.0394 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0771 - val_loss: 1.0615 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0145 - val_loss: 1.0378 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8801 - val_loss: 1.0302 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9192 - val_loss: 1.0347 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8754 - val_loss: 1.0186 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8055 - val_loss: 1.0130 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7471 - val_loss: 1.0334 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7289 - val_loss: 1.0340 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7415 - val_loss: 1.0231 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7079 - val_loss: 1.0281 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6107 - val_loss: 1.0328 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6286 - val_loss: 1.0572 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m46/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5802\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5851 - val_loss: 1.0857 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5584 - val_loss: 1.0480 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5620 - val_loss: 1.0585 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5123 - val_loss: 1.0572 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5028 - val_loss: 1.0522 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5525 - val_loss: 1.0596 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5316 - val_loss: 1.0481 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m51/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5146\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5149 - val_loss: 1.0742 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4930 - val_loss: 1.0663 - learning_rate: 2.5000e-04\n",
      "Making predictions...\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Writing output position file\n",
      "Total time consuming = 91.208s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Read in the configuration file\n",
    "def read_cfg_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_fmt = [line.rstrip('\\n').split(' ') for line in lines]\n",
    "    info = line_fmt\n",
    "    bs_pos = list([float(info[0][0]), float(info[0][1]), float(info[0][2])])\n",
    "    tol_samp_num = int(info[1][0])\n",
    "    anch_samp_num = int(info[2][0])\n",
    "    port_num = int(info[3][0])\n",
    "    ant_num = int(info[4][0])\n",
    "    sc_num = int(info[5][0])\n",
    "    return bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num\n",
    "\n",
    "# Read in the info related to the anchor points\n",
    "def read_anch_file(file_path, anch_samp_num):\n",
    "    anch_pos = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_fmt = [line.rstrip('\\n').split(' ') for line in lines]\n",
    "    for line in line_fmt:\n",
    "        tmp = np.array([int(line[0]), float(line[1]), float(line[2])])\n",
    "        if np.size(anch_pos) == 0:\n",
    "            anch_pos = tmp\n",
    "        else:\n",
    "            anch_pos = np.vstack((anch_pos, tmp))\n",
    "    return anch_pos\n",
    "\n",
    "# The channel file is large, read in channels in smaller slices\n",
    "def read_slice_of_file(file_path, start, end):\n",
    "    with open(file_path, 'r') as file:\n",
    "        slice_lines = list(itertools.islice(file, start, end))\n",
    "    return slice_lines\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"<<< Welcome to 2024 Wireless Algorithm Contest! >>>\\n\")\n",
    "    ## For ease of data managenment, input data for different rounds are stored in different folders. Feel free to define your own\n",
    "    # PathSet = {0: \"./Test\", 1: \"./Dataset0\", 2: \"./CompetitionData2\", 3: \"./CompetitionData3\"}\n",
    "    # PrefixSet = {0: \"Round0\", 1: \"Round1\", 2: \"Round2\", 3: \"Round3\"}\n",
    "    PathSet = {0: \"./Test\", 1: \"./Dataset0\", 2: \"../Dataset1\", 3: \"./Dataset2\"}\n",
    "    PrefixSet = {1: \"Dataset0\", 2: \"Dataset1\", 3: \"Dataset2\"}\n",
    "\n",
    "    Ridx = 2  # Flag defining the round of the competition, used for define where to read data。0:Test; 1: 1st round; 2: 2nd round ...\n",
    "    PathRaw = PathSet[Ridx]\n",
    "    Prefix = PrefixSet[Ridx]\n",
    "    \n",
    "    ### Get all files in the folder related to the competition. Data for other rounds should be kept in a different folder  \n",
    "    files = os.listdir(PathRaw)\n",
    "    names = []\n",
    "    for f in sorted(files):\n",
    "        if f.find('CfgData') != -1 and f.endswith('.txt'):\n",
    "            names.append(f.split('CfgData')[-1].split('.txt')[0])\n",
    "    \n",
    "    \n",
    "    for na in names:\n",
    "        FileIdx = int(na)\n",
    "        print('Processing Round ' + str(Ridx - 1) + ' Case ' + str(na))\n",
    "        \n",
    "        # Read in the configureation file: RoundYCfgDataX.txt\n",
    "        # print('Loading configuration data file')\n",
    "        cfg_path = PathRaw + '/' + Prefix + 'CfgData' + na + '.txt'\n",
    "        bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num = read_cfg_file(cfg_path)\n",
    "                \n",
    "        # Read in info related to the anchor points: RoundYInputPosX.txt\n",
    "        # print('Loading input position file')\n",
    "        anch_pos_path = PathRaw + '/' + Prefix + 'InputPos' + na + '.txt'\n",
    "        anch_pos = read_anch_file(anch_pos_path, anch_samp_num)\n",
    "\n",
    "        # Read in channel data:  RoundYInputDataX.txt\n",
    "        # slice_samp_num = 1000  # number of samples in each slice\n",
    "        # slice_num = int(tol_samp_num / slice_samp_num)  # total number of slices\n",
    "        # csi_path = PathRaw + '/' + Prefix + 'InputData' + na + '.txt'\n",
    "        # H = []\n",
    "        # for slice_idx in range(slice_num): # Read in channel data in a loop. In each loop, only one slice of channel is read in\n",
    "        #     print('Loading input CSI data of slice ' + str(slice_idx))\n",
    "        #     slice_lines = read_slice_of_file(csi_path, slice_idx * slice_samp_num, (slice_idx + 1) * slice_samp_num)\n",
    "        #     Htmp = np.loadtxt(slice_lines)\n",
    "        #     Htmp = np.reshape(Htmp, (slice_samp_num, 2, sc_num, ant_num, port_num))\n",
    "        #     Htmp = Htmp[:, 0, :, :, :] + 1j * Htmp[:, 1, :, :, :]\n",
    "        #     Htmp = np.transpose(Htmp, (0, 3, 2, 1))  # Htmp: (slice_samp_num, ant_num, sc_num, port_num)\n",
    "        #     if np.size(H) == 0:\n",
    "        #         H = Htmp\n",
    "        #     else:\n",
    "        #         H = np.concatenate((H, Htmp), axis=0)\n",
    "        # H = H.astype(np.complex64) # trunc to complex64 to reduce storage\n",
    "        \n",
    "        csi_file = PathRaw + '/' + Prefix + 'InputData' + na + '.npy'\n",
    "        # np.save(csi_file, H) # After reading the file, you may save txt file into npy, which is faster for python to read \n",
    "        H = np.load(csi_file) # if saved in npy, you can load npy file instead of txt\n",
    "        \n",
    "        tStart = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        print('Calculating localization results')\n",
    "        result = calcLoc(H, anch_pos, bs_pos, tol_samp_num, anch_samp_num, port_num, ant_num, sc_num) # This function should be implemented by yourself\n",
    "        \n",
    "        # Replace the position information for anchor points with ground true coordinates\n",
    "        for idx in range(anch_samp_num):\n",
    "            rowIdx = int(anch_pos[idx][0] - 1)\n",
    "            result[rowIdx] = np.array([anch_pos[idx][1], anch_pos[idx][2]])\n",
    "\n",
    "        # Output, be careful with the precision\n",
    "        print('Writing output position file')\n",
    "        with open(PathRaw + '/Outputs' + '/' + Prefix + 'Output' + na + '.txt', 'w') as f:\n",
    "            np.savetxt(f, result, fmt='%.4f %.4f')\n",
    "\n",
    "        # This help to evaluate the running time, can be removed!\n",
    "        tEnd = time.perf_counter()\n",
    "        print(\"Total time consuming = {}s\\n\\n\".format(round(tEnd - tStart, 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
